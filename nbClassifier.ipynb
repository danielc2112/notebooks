{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DOCUMENT FILTERING</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código deste exemplo se baseia no exemplo de **_Segaran_ 2007**: \"Programming Collective Intelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao classificar documentos -- precisaremos de algumas coisas:\n",
    "\n",
    "i) _features_ i.e. qualquer coisa que você pode determinar como *presente* ou *ausente* no documento, ou que você possa *contar* nos documentos;\n",
    "<br>ii) _classes_; se não estivermos fazendo _Topic Modeling_ as classes que queremos usar, muitas vezes, são dadas (e.g. fraudulento vs. normal; CV desejável vs. não; SPAM vs HAM; etc) \n",
    "<br>iii) bem ...mais documentos. A maioria dos modelos de classificação são **supervisionados** (todo mundo sabe a diferença? -- senão estará na próxima aula!), logo é preciso **treinar** o classificador.\n",
    "\n",
    "Bem, ao considerar documentos para classificação, um bom candidato para _features_ são as palavras do documento. Outros incluem: as entidades nomeadas (named-entities); os meta-dados associados ou qualquer coisa que sirva como _feature_\n",
    "<br>\n",
    "mas para este exemplo, vamos com as palavras:\n",
    "\n",
    "Criamos uma função em python que extrai as palavras de um documento:\n",
    "\n",
    "<h4>OBS.: O código em si não importa tanto quando os conceitos!</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import re\n",
    "\n",
    "def getwords(doc):\n",
    "    splitter = re.compile('\\\\W*')\n",
    "    # Split the words by non-alpha characters\n",
    "    words = [s.lower() for s in splitter.split(doc) if len(s) > 2 and len(s) < 20]\n",
    "    \n",
    "    #print words  # usamos isso para checar o split depois\n",
    "    \n",
    "    # retorno o set de palavras ÚNICAS!\n",
    "    res = dict([(w, 1) for w in words])\n",
    "    # print 'res:',res # veja\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta função divide um _string_ partindo em cada coisa que não é uma letra e convertendo para minúsculo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBS SOBRE A ESCOLHA DE _FEATURES_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A escolha das _features_ é muito importante. E difícil. Os documentos precisam ter algumas _features_ em comum, mas uma _feature_ que ocorre em **todo** documento é inútil para a classificação. \n",
    "\n",
    "Em tese o texto inteiro de cada documento poderia ser uma feature. Mas aí teriamos um classificador que coloca cada um documento em uma classe separada (a não ser que haja documentos exatamente iguais). Do outro lado do espectro, podemos usar as letras como _features_, mas aí como todos os documentos de um _corpus_ em geral tendem a ter o mesmo alfabeto, elas não seriam uma forma efetiva de separar documentos.\n",
    "\n",
    "até **quais** palavras utilizar pode ser problemático, usamos pontuação ou não? quais as melhores regras para dividir palavras? Usamos o título (em caso de artigos ou notícias)? Colocar em minúsculo perde a informação de nomes próprios, inícios de frase, acrônimos.\n",
    "\n",
    "Por exemplo -- para um detector de SPAM ...o estilo 'GRITADO' DE DIGITAR É CRUCIAL PARA DISTIGUIR MENSAGENS, E O NÚMERO DE EXCLAMAÇÕES TAMBÉM!!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAMOS TREINAR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso em mente, vamos programar e treinar o nosso classificador. Classificadores supervisionados melhoram à medida que vêem mais e mais documentos (tomando cuidado com _overfitting/overtraining_).\n",
    "\n",
    "<h4>Todo mundo sabe o que é overfitting/overtraining?? Senão, veremos na próxima aula!</h4>\n",
    "\n",
    "Vamos escrever uma classe genérica que descreve um classificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier:\n",
    "    \"\"\" um classificador genérico que serve para trinar um Naïve Bayes \"\"\"\n",
    "    def __init__(self,  getfeatures,  filename=None):\n",
    "        # contagem de combinações: feature/categoria\n",
    "        # ex: {'python': {'bad': 0, 'good': 6}, 'the': {'bad': 3, 'good': 3}}\n",
    "        self.featCatCombinations = {}\n",
    "        \n",
    "        # contagem de documentos em cada cat.\n",
    "        # ex: {'good': 385, 'bad':266}\n",
    "        self.docCountPerCat = {}\n",
    "        \n",
    "        # fn. de extração de features (recebida como input)\n",
    "        # no nosso caso é a getWords\n",
    "        self.getfeatures = getfeatures\n",
    "        \n",
    "        # veremos mais em baixo para que isso serve!\n",
    "        self.thresholds = {}\n",
    "        \n",
    "    # #################################################################\n",
    "    # estes são helper methods para que nossa classe permaneça genérica\n",
    "    # #################################################################\n",
    "    def incrFeatCount(self,  f,  cat):\n",
    "        \"\"\"Incrementa a contagem da feature f na categoria cat\"\"\"\n",
    "        self.featCatCombinations.setdefault(f, {})\n",
    "        self.featCatCombinations[f].setdefault(cat, 0)\n",
    "        self.featCatCombinations[f][cat] += 1\n",
    "\n",
    "    def incrCatCount(self,  cat):\n",
    "        \"\"\"Incr. a contagem de uma cat\"\"\"\n",
    "        self.docCountPerCat.setdefault(cat, 0)\n",
    "        self.docCountPerCat[cat] += 1\n",
    "\n",
    "    def fcount(self,  f,  cat):\n",
    "        \"\"\"num. de vezes uma feature aparece em uma cat\"\"\"\n",
    "        if f in self.featCatCombinations and cat in self.featCatCombinations[f]:\n",
    "            return float(self.featCatCombinations[f][cat])\n",
    "        return 0.0\n",
    "\n",
    "    def catcount(self,  cat):\n",
    "        \"\"\"num. de itens em uma cat.\"\"\"\n",
    "        if cat in self.docCountPerCat:\n",
    "            return float(self.docCountPerCat[cat])\n",
    "        return 0\n",
    "\n",
    "    def totalcount(self):\n",
    "        \"\"\"num. total de itens\"\"\"\n",
    "        return sum(self.docCountPerCat.values())\n",
    "    \n",
    "    def categories(self):\n",
    "        \"\"\"lista de categorias\"\"\"\n",
    "        return self.docCountPerCat.keys()\n",
    "    # #################################################################     \n",
    "    def train(self, item, cat):\n",
    "        \"\"\"pego um documento classificado, parto ele em features e \n",
    "            adiciono as contagens deste doc. ao todo\"\"\"\n",
    "        features = self.getfeatures(item)\n",
    "        # incr. a contagem para cada feature na cat.\n",
    "        for f in features:\n",
    "            self.incrFeatCount(f, cat)\n",
    "        # incr. a contagem na cat.\n",
    "        self.incrCatCount(cat)\n",
    "    # #################################################################            \n",
    "    def fprob(self, f, cat):\n",
    "        \"\"\"probabilidade de uma feature >F< ocorrer na categoria >C<:\n",
    "           i.e: num. de vezes F aparece em C sobre o num de itens em C\n",
    "        \"\"\"\n",
    "        if self.catcount(cat) == 0: return 0  # se vazio, retorna 0\n",
    "        return self.fcount(f, cat)/self.catcount(cat)\n",
    "    \n",
    "    def weightedprob(self, f, cat, prf, weight=1.0, ap=0.5):\n",
    "        \"\"\"probabilidade ponderada (veja abaixo)\n",
    "            weight é o peso (em qtd de palavras) que a prob assumida tem\n",
    "            ap é a probabilidade assumida (assumed probability - ap)\n",
    "        \"\"\"\n",
    "        # calcular a probabilidade basica\n",
    "        basicprob = prf(f, cat)\n",
    "    \n",
    "        # contar o numero de vezes que a feature aparece em TODAS as categorias\n",
    "        totalOcc = sum([self.fcount(f, c) for c in self.categories()])\n",
    "        \n",
    "        # calcular a probabilidade ponderada\n",
    "        bp = ((weight * ap) + (totalOcc * basicprob)) / (weight + totalOcc)\n",
    "        #  = (   1  *  0.5) + (  soma   * prAssumida) / (  1    +   soma  )\n",
    "        \n",
    "        return bp\n",
    "    # ################################################################# \n",
    "    # setter e getter para as thesholds de cada categoria, veremos abaixo\n",
    "    # ################################################################# \n",
    "    def setthreshold(self, cat, t):\n",
    "        self.thresholds[cat] = t\n",
    "    \n",
    "    def getthreshold(self, cat):\n",
    "        if cat not in self.thresholds: return 1.0\n",
    "        return self.thresholds[cat]\n",
    "    \n",
    "    # #################################################################\n",
    "    # aqui finalmente \n",
    "    def classify(self, item, default=None):\n",
    "        probs = {}\n",
    "        # Encontra a classe com a maior probabilidade\n",
    "        max = 0.0\n",
    "        for cat in self.categories():\n",
    "            probs[cat] = self.prob(item, cat)\n",
    "            if probs[cat] > max:\n",
    "                max = probs[cat]\n",
    "                best = cat\n",
    "        # garante que a probabilidade excede threshold*next best\n",
    "        for cat in probs:\n",
    "            if cat == best: continue\n",
    "            if probs[cat] * self.getthreshold(best) > probs[best]: return default\n",
    "        return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos checar os helpers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=classifier(getwords)\n",
    "cl.train('the quick brown quick fox jumps over the lazy dog','good')\n",
    "cl.train('make quick money in the online casino','bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que a palavra 'the' só aparece uma vez, em um documento bom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.fcount('the','good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "igualmente, 'quick' só aparece uma vez em um documento classificado como mau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.fcount('quick','bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Agora podemos criar uma função place-holder para um _corpus_ de e-mails; e vamos tentar achar SPAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampletrain(cl):\n",
    "    cl.train('Nobody owns the water.', 'good')\n",
    "    cl.train('the quick rabbit jumps fences', 'good')\n",
    "    cl.train('buy pharmaceuticals now', 'bad')\n",
    "    cl.train('make quick money at the online casino', 'bad')\n",
    "    cl.train('Mike is quick to store his money in bonds','good')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Adicionamos à nossa classe funções de contagem, agora vamos extrair probabilidades: A função <code>fprob</code> faz exatamente isso, veja lá.\n",
    "\n",
    "Zeramos o classificador e treinamos no _corpus_ anotado (já classificado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl=classifier(getwords)\n",
    "sampletrain(cl)\n",
    "cl.fprob('quick','good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso se chama _probabilidade condicional_: Pr(A | B). \n",
    "\n",
    "Neste exemplo temos a probabilidade Pr(palavra | classe), isto é, para uma dada classificação, calculamos a probabilidade de uma dada palavra aparecer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UM BOM CHUTE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>fprob</code> tem um leve problema, que é usar a informação que foi vista até um dado momento torna o modelo **sensível** durante o início do treino e a palavras que ocorrem **raramente**. 'Money' aparece somente uma vez e é classificada como ruim pois aparece em um anúncio de casino, mas 'money' pode ser (e provavelmente é) uma palavra neutra em um corpo de e-mails genérico.\n",
    "\n",
    "Com isso, a probabilidade de 'money' aparecer em 'good' agora é **zero**. Seria muito mais realistico se o valor gradualmente se aproximasse de zero à medida que mais e mais documentos fossem vistos.\n",
    "\n",
    "Assim, vamos partir de uma probabilidade assumida (0.5, por exemplo) e um peso para esta probabilidade: um peso de 1 significa que ela vale **uma palavra**, assim a nossa probabilidade com pesos (<code>weighedprob</code> lá em cima) será uma média ponderada entre a palavra (<code>get</code>) e a probabilidade assumida).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAÏVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que temo as probabilidades de cada categoria conter uma certa palavra, podemos combinar as probabilidades de palavras individuais para ter a probabilidade de um documento pertencer a uma dada classe.\n",
    "\n",
    "Este método se chama 'naïve' (ingênuo) pois parte da premissa que as probabilidades sendo combinadas são **independentes** (por isso multiplicamos) – isto é – assumimos que a probabilidade de uma palavra de um documento ocorrer em uma categoria é **independente** de todas as outras palavras serem ou não daquela categoria\n",
    "\n",
    "<br><center>**Isso não é verdade, Né?**</center><br>\n",
    "\n",
    "A palavra 'cassino' deve ocorrer junto com 'ganhar' muito mais que com 'alumínio' ou 'feijoada' (pelo menos eu espero).\n",
    "\n",
    "Para um classificador Naive Bayes, precisamos da probabilidade de um documento inteiro ser dado uma classificação. Por assumir independência, podemos somente multiplicar as probabilidades individuais das palavras para obter a do documento. Veja <code>docprob</code> abaixo.\n",
    "\n",
    "\n",
    "Agora sabemos calcular $Pr(Documento | Classe)$, mas isso não serve de muito.\n",
    "\n",
    "Para classificar documentos precisamos de $Pr(Classe | Documento)$, ou seja, dado o documento qual a probabilidade de ele pertencer a uma classe específica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teorema de Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entra o Teorema de Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(A|B) = \\frac{Pr(B|A)Pr(A)}{Pr(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou seja: $$Pr(Classe | Documento) = \\frac{Pr(Documento | Classe) \\cdot Pr(Classe)}{Pr(Documento)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, $Pr(Classe)$ é a probabilidade que um documento aleatório será desta classe, então é só o número de documentos na classe divido pelo número total de documentos.\n",
    "\n",
    "$Pr(Documento)$ é desnecessário pois os resultados não serão usados em uma probabilidade, e sim apenas comparativamente entre classes (e todas terão o mesmo denominador), então podemos ignorá-lo!\n",
    "\n",
    "O método <code>prob</code> calcula a probabilidade da categoria e retorna o produto de $Pr(Document|Category)$ e $Pr(Category)$. Vamos adicionar isto à classe NaiveBayes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(classifier):\n",
    "    \n",
    "    def docprob(self,  item,  cat):\n",
    "        \"\"\"pega a probabilidade multiplicada de cada feature (palavra) do documento\"\"\"\n",
    "        # pegar as feature\n",
    "        features = self.getfeatures(item)\n",
    "        # Mutiplico as probabilidades de cada feature\n",
    "        p = 1\n",
    "        for f in features:\n",
    "            p *= self.weightedprob(f, cat, self.fprob)\n",
    "        return p\n",
    "    \n",
    "    def prob(self, item, cat):\n",
    "        \n",
    "        # calculando Pr(Classe)\n",
    "        catprob = self.catcount(cat) / self.totalcount()\n",
    "        \n",
    "        # calculando Pr(Doc | Classe)\n",
    "        docprob = self.docprob(item, cat)\n",
    "        \n",
    "        # retornando Pr( Classe | Doc) = Pr(Doc | Classe) * Pr(Classe)\n",
    "        return docprob*catprob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo uma categoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ultimo passo é escolher uma classe para o documento. A forma mais simples seria calcular as probabilidades de cada classe e escolher a maior.\n",
    "\n",
    "Se estivermos apenas tentando descobrir a classe mais apropriada para um item esta é uma estratégia que se aplica, mas em muitas aplicações é melhor o classificador admitir que **não sabe** a resposta do que decidir que a resposta é uma categoria com uma probabilidade marginalmente maior.\n",
    "\n",
    "No nosso exemplo de SPAM, é muito mais importante **<u>evitar que um email bom seja classificado como spam</u>** do que pegar absolutamente todos os spams. Uma mensagem de spam na _inbox_ de vez em quando pode ser tolerada, mas um email importante que é jogado na caixa de spam pode ser negligenciado completamente. Se você precisa olhar no seu folder de spam procurando e-mails importantes, não há razão para _ter_ um folder de spam. \n",
    "\n",
    "Para lidar com isso, podemos colocar um _threshold_, isto é, um valor mínimo para cada categoria. Para que um novo documento caia em uma categoria, ela precisa ser >x< mais provável que qualquer outra, digamos 3. \n",
    "\n",
    "Assim, qualquer mensagem cuja probabilidade é alta ..mas não 3 vezes mais alta de ser spam é classificada como 'unknown'\n",
    "\n",
    "vamos ver no código... lé em cima!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fprob for \"money\" being good 0.333333333333\n",
      "fprob for \"money\" being bad 0.5\n",
      "\n",
      "weightedprob for \"money\" being good 0.388888888889\n",
      "weightedprob for \"money\" being bad 0.5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    cl = classifier(getwords)\n",
    "    sampletrain(cl)\n",
    "    print 'fprob for \"money\" being good', cl.fprob('money', 'good')\n",
    "    print 'fprob for \"money\" being bad', cl.fprob('money', 'bad')\n",
    "\n",
    "    print '\\nweightedprob for \"money\" being good', cl.weightedprob('money', 'good', cl.fprob)\n",
    "    print 'weightedprob for \"money\" being bad', cl.weightedprob('money', 'bad', cl.fprob)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "retraining on new text:\t\t\"money, that's what I want\"\n",
      "\n",
      "weightedprob for \"money\" being good 0.375\n",
      "weightedprob for \"money\" being bad 0.625\n"
     ]
    }
   ],
   "source": [
    "    s = \"money, that's what I want\"\n",
    "    print '\\nretraining on new text:\\t\\t','\"'+s+'\"\\n'\n",
    "    cl.train(s, \"bad\")\n",
    "\n",
    "    print 'weightedprob for \"money\" being good', cl.weightedprob('money', 'good', cl.fprob)\n",
    "    print 'weightedprob for \"money\" being bad', cl.weightedprob('money', 'bad', cl.fprob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Agora vamos testar o nosso classificador</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "is 'quick' good? 0.375\n",
      "is 'quick'  bad? 0.2\n",
      "it is classified as: good\n",
      "\n",
      "retrainting on two new texts...\n",
      "\t\t\"Take this other quick quiz and make money!\"\n",
      "\t\t\"click here quick! Woman want to talk to you!\"\n",
      "\n",
      "is 'quick' good? 0.27380952381\n",
      "is 'quick' bad? 0.404761904762\n",
      "now it is: bad\n",
      "\n",
      "\n",
      "\n",
      "agora vamos classificar o um novo texto que recebemos:\n",
      "\t\t\"my dog, while quick, cannot jump\"\n",
      "\n",
      "bad\n",
      "e com threshold: unknown\n"
     ]
    }
   ],
   "source": [
    "    nbClassifier = NaiveBayes(getwords)\n",
    "    sampletrain(nbClassifier)\n",
    "    \n",
    "    print \"\\nis 'quick' good?\", nbClassifier.prob('quick', 'good')\n",
    "    print \"is 'quick'  bad?\",nbClassifier.prob('quick', 'bad')\n",
    "    \n",
    "    print \"it is classified as:\",nbClassifier.classify('quick', default='unknown')\n",
    "    \n",
    "    s2 = \"Take this other quick quiz and make money!\"\n",
    "    s3 = \"click here quick! Woman want to talk to you!\"\n",
    "    print '\\nretrainting on two new texts...'\n",
    "    print '\\t\\t\"'+s2+'\"'\n",
    "    print '\\t\\t\"'+s3+'\"' \n",
    "    nbClassifier.train(s2, \"bad\")\n",
    "    nbClassifier.train(s3, \"bad\")\n",
    "    \n",
    "    print \"\\nis 'quick' good?\", nbClassifier.prob('quick', 'good')\n",
    "    print \"is 'quick' bad?\",nbClassifier.prob('quick', 'bad')\n",
    "    print 'now it is:',nbClassifier.classify('quick', default='unknown')\n",
    "    \n",
    "    print '\\n\\n'\n",
    "    \n",
    "    sNew = 'my dog, while quick, cannot jump'\n",
    "    #sNew = 'my rabbit, while quick, cannot jump'\n",
    "    print 'agora vamos classificar o um novo texto que recebemos:'\n",
    "    print '\\t\\t\"'+sNew+'\"\\n'\n",
    "    print nbClassifier.classify(sNew, default='unknown')\n",
    "    \n",
    "    print 'e com threshold:',\n",
    "    nbClassifier.setthreshold('bad', 3)\n",
    "    print nbClassifier.classify(sNew, default='unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Concluindo</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma razão que classificadores Bayesianos são usados para classificação é pois eles requerem **muito menos computação** que outros métodos. Um texto ou mensagem pode ser enorme, contendo até milhares de palavras, e simplesmente atualizar contagens toma _muit_ menos memória e ciclos de processador que, por exemplo, treinar uma rede neural do tamanho necessário.\n",
    "\n",
    "Uma rede neural, neste caso, também tem a desvantagem de não ser **interpretável**. Neste exemplo podemos olhar as probabilidades individuais e como elas contribuem ao todo, enquanto os pesos de conexão em uma rede neural não permitem interpretação direta.\n",
    "\n",
    "Por outro lado, redes neurais e classificadores como SVMs pode capturar relações mais complexas entre as _features_. Em uma ANN a probabilidade de uma _feature_ pode mudar em resposta à presença ou ausência de outras _features_. Por exemplo, podemos querer filtrar a  palavra \"cassino\" a não ser que apareça junto com a palavra \"filme\" \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
